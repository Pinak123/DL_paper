{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:18.347479Z","iopub.execute_input":"2025-07-31T20:53:18.347754Z","iopub.status.idle":"2025-07-31T20:53:22.476665Z","shell.execute_reply.started":"2025-07-31T20:53:18.347733Z","shell.execute_reply":"2025-07-31T20:53:22.476116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        # pe is automatically on the correct device due to register_buffer\n        return x + self.pe[:, :x.size(1)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.477773Z","iopub.execute_input":"2025-07-31T20:53:22.478110Z","iopub.status.idle":"2025-07-31T20:53:22.483755Z","shell.execute_reply.started":"2025-07-31T20:53:22.478089Z","shell.execute_reply":"2025-07-31T20:53:22.483054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multihead Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        Q = self.W_q(query)\n        K = self.W_k(key)\n        V = self.W_v(value)\n        \n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model)\n        \n        output = self.W_o(attn_output)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.484308Z","iopub.execute_input":"2025-07-31T20:53:22.484467Z","iopub.status.idle":"2025-07-31T20:53:22.506803Z","shell.execute_reply.started":"2025-07-31T20:53:22.484454Z","shell.execute_reply":"2025-07-31T20:53:22.506201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.508330Z","iopub.execute_input":"2025-07-31T20:53:22.508515Z","iopub.status.idle":"2025-07-31T20:53:22.529708Z","shell.execute_reply.started":"2025-07-31T20:53:22.508500Z","shell.execute_reply":"2025-07-31T20:53:22.528859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoder Layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.530532Z","iopub.execute_input":"2025-07-31T20:53:22.530844Z","iopub.status.idle":"2025-07-31T20:53:22.546548Z","shell.execute_reply.started":"2025-07-31T20:53:22.530821Z","shell.execute_reply":"2025-07-31T20:53:22.546024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decoder Layer","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.547204Z","iopub.execute_input":"2025-07-31T20:53:22.547427Z","iopub.status.idle":"2025-07-31T20:53:22.566346Z","shell.execute_reply.started":"2025-07-31T20:53:22.547407Z","shell.execute_reply":"2025-07-31T20:53:22.565918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, \n                 num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n        \n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) \n            for _ in range(num_layers)\n        ])\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout) \n            for _ in range(num_layers)\n        ])\n        \n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        # Get device from input tensors\n        device = src.device\n        \n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        \n        # Create nopeak_mask on the same device as input tensors\n        nopeak_mask = (1 - torch.triu(\n            torch.ones(1, seq_length, seq_length, device=device), \n            diagonal=1\n        )).bool()\n        \n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        \n        src_embedded = self.dropout(self.positional_encoding(\n            self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(\n            self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.567254Z","iopub.execute_input":"2025-07-31T20:53:22.567441Z","iopub.status.idle":"2025-07-31T20:53:22.583216Z","shell.execute_reply.started":"2025-07-31T20:53:22.567426Z","shell.execute_reply":"2025-07-31T20:53:22.582539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Handling Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Check available files\n# print(os.listdir('/kaggle/input/hindi-english-parallel-corpus/'))\n\n# Load your data\ndf = pd.read_csv('/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:22.583868Z","iopub.execute_input":"2025-07-31T20:53:22.584097Z","iopub.status.idle":"2025-07-31T20:53:33.055019Z","shell.execute_reply.started":"2025-07-31T20:53:22.584081Z","shell.execute_reply":"2025-07-31T20:53:33.054129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport re\n\nclass TranslationCSVDataset(Dataset):\n    def __init__(self, csv_file, src_col, tgt_col, src_tokenizer, tgt_tokenizer, \n                 max_length=512, test_size=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with translation data\n            src_col (string): Column name for source language\n            tgt_col (string): Column name for target language\n            src_tokenizer: Tokenizer for source language\n            tgt_tokenizer: Tokenizer for target language\n            max_length (int): Maximum sequence length\n            test_size (float): If provided, will split data into train/test\n        \"\"\"\n        # Load CSV data\n        self.df = pd.read_csv(csv_file)\n        \n        # Clean and filter data\n        self.df = self.df.dropna(subset=[src_col, tgt_col])\n        self.df = self.df[self.df[src_col].str.len() > 0]\n        self.df = self.df[self.df[tgt_col].str.len() > 0]\n        \n        # Split data if test_size is provided\n        if test_size:\n            self.train_df, self.test_df = train_test_split(\n                self.df, test_size=test_size, random_state=42\n            )\n            self.df = self.train_df  # Use training data by default\n        \n        self.src_col = src_col\n        self.tgt_col = tgt_col\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.max_length = max_length\n        \n        print(f\"Loaded {len(self.df)} translation pairs\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        src_text = str(row[self.src_col]).strip()\n        tgt_text = str(row[self.tgt_col]).strip()\n        \n        # Clean text (optional preprocessing)\n        src_text = self.clean_text(src_text)\n        tgt_text = self.clean_text(tgt_text)\n        \n        # Tokenize\n        src_tokens = self.src_tokenizer.encode(src_text)[:self.max_length-2]\n        tgt_tokens = self.tgt_tokenizer.encode(tgt_text)[:self.max_length-2]\n        \n        # Add special tokens (1=BOS, 2=EOS, 0=PAD)\n        src_tokens = [1] + src_tokens + [2]\n        tgt_tokens = [1] + tgt_tokens + [2]\n        \n        # Pad sequences\n        src_tokens += [0] * (self.max_length - len(src_tokens))\n        tgt_tokens += [0] * (self.max_length - len(tgt_tokens))\n        \n        return {\n            'src': torch.tensor(src_tokens, dtype=torch.long),\n            'tgt': torch.tensor(tgt_tokens, dtype=torch.long),\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }\n    \n    def clean_text(self, text):\n        \"\"\"Basic text cleaning\"\"\"\n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text)\n        # Remove special characters if needed (adjust based on your data)\n        # text = re.sub(r'[^\\w\\s.,!?]', '', text)\n        return text.strip()\n    \n    def get_test_data(self):\n        \"\"\"Return test dataset if split was performed\"\"\"\n        if hasattr(self, 'test_df'):\n            test_dataset = TranslationCSVDataset.__new__(TranslationCSVDataset)\n            test_dataset.df = self.test_df\n            test_dataset.src_col = self.src_col\n            test_dataset.tgt_col = self.tgt_col\n            test_dataset.src_tokenizer = self.src_tokenizer\n            test_dataset.tgt_tokenizer = self.tgt_tokenizer\n            test_dataset.max_length = self.max_length\n            return test_dataset\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:33.055805Z","iopub.execute_input":"2025-07-31T20:53:33.056024Z","iopub.status.idle":"2025-07-31T20:53:33.711012Z","shell.execute_reply.started":"2025-07-31T20:53:33.056007Z","shell.execute_reply":"2025-07-31T20:53:33.710385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom collections import Counter\n\nclass SimpleTokenizer:\n    def __init__(self, vocab_size=10000):\n        self.vocab_size = vocab_size\n        self.word2idx = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n        self.idx2word = {0: '<PAD>', 1: '<BOS>', 2: '<EOS>', 3: '<UNK>'}\n        self.vocab_built = False\n    \n    def build_vocab(self, texts):\n        \"\"\"Build vocabulary from list of texts\"\"\"\n        word_counts = Counter()\n        \n        for text in texts:\n            words = text.lower().split()\n            word_counts.update(words)\n        \n        # Add most common words to vocabulary\n        most_common = word_counts.most_common(self.vocab_size - 4)  # -4 for special tokens\n        \n        for i, (word, _) in enumerate(most_common):\n            if word not in self.word2idx:\n                idx = len(self.word2idx)\n                self.word2idx[word] = idx\n                self.idx2word[idx] = word\n        \n        self.vocab_built = True\n        print(f\"Built vocabulary with {len(self.word2idx)} words\")\n    \n    def encode(self, text):\n        \"\"\"Convert text to list of token IDs\"\"\"\n        if not self.vocab_built:\n            raise ValueError(\"Vocabulary not built. Call build_vocab() first.\")\n        \n        words = text.lower().split()\n        return [self.word2idx.get(word, 3) for word in words]  # 3 is <UNK>\n    \n    def decode(self, token_ids):\n        \"\"\"Convert list of token IDs back to text\"\"\"\n        words = [self.idx2word.get(idx, '<UNK>') for idx in token_ids \n                if idx not in [0, 1, 2]]  # Skip PAD, BOS, EOS\n        return ' '.join(words)\n    \n    def save(self, filepath):\n        \"\"\"Save tokenizer to file\"\"\"\n        with open(filepath, 'wb') as f:\n            pickle.dump({\n                'word2idx': self.word2idx,\n                'idx2word': self.idx2word,\n                'vocab_size': self.vocab_size\n            }, f)\n    \n    def load(self, filepath):\n        \"\"\"Load tokenizer from file\"\"\"\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n            self.word2idx = data['word2idx']\n            self.idx2word = data['idx2word']\n            self.vocab_size = data['vocab_size']\n            self.vocab_built = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:33.713006Z","iopub.execute_input":"2025-07-31T20:53:33.713342Z","iopub.status.idle":"2025-07-31T20:53:33.722034Z","shell.execute_reply.started":"2025-07-31T20:53:33.713324Z","shell.execute_reply":"2025-07-31T20:53:33.721140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_translation_dataloaders(csv_file, src_col='source', tgt_col='target',\n                                 batch_size=32, max_length=128, test_size=0.1):\n    \"\"\"\n    Create DataLoaders for translation training from CSV file\n    \"\"\"\n    \n    # Step 1: Load data and build tokenizers\n    print(\"Loading data and building vocabularies...\")\n    df = pd.read_csv(csv_file)\n    df = df.dropna(subset=[src_col, tgt_col])\n    \n    # Extract texts for vocabulary building\n    src_texts = df[src_col].tolist()\n    tgt_texts = df[tgt_col].tolist()\n    \n    # Build tokenizers\n    src_tokenizer = SimpleTokenizer(vocab_size=10000)\n    tgt_tokenizer = SimpleTokenizer(vocab_size=10000)\n    \n    src_tokenizer.build_vocab(src_texts)\n    tgt_tokenizer.build_vocab(tgt_texts)\n    \n    # Save tokenizers\n    src_tokenizer.save('src_tokenizer.pkl')\n    tgt_tokenizer.save('tgt_tokenizer.pkl')\n    \n    # Step 2: Create datasets\n    dataset = TranslationCSVDataset(\n        csv_file=csv_file,\n        src_col=src_col,\n        tgt_col=tgt_col,\n        src_tokenizer=src_tokenizer,\n        tgt_tokenizer=tgt_tokenizer,\n        max_length=max_length,\n        test_size=test_size\n    )\n    \n    # Step 3: Create DataLoaders\n    train_loader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # Create test loader if test data exists\n    test_dataset = dataset.get_test_data()\n    test_loader = None\n    if test_dataset:\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n    \n    return train_loader, test_loader, src_tokenizer, tgt_tokenizer\n\n# Custom collate function for batching\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle batching\"\"\"\n    src_batch = torch.stack([item['src'] for item in batch])\n    tgt_batch = torch.stack([item['tgt'] for item in batch])\n    \n    return {\n        'src': src_batch,\n        'tgt': tgt_batch,\n        'src_texts': [item['src_text'] for item in batch],\n        'tgt_texts': [item['tgt_text'] for item in batch]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:53:33.722768Z","iopub.execute_input":"2025-07-31T20:53:33.722930Z","iopub.status.idle":"2025-07-31T20:53:33.747447Z","shell.execute_reply.started":"2025-07-31T20:53:33.722917Z","shell.execute_reply":"2025-07-31T20:53:33.746853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your CSV data\ncsv_file = '/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv'  # Replace with your file path\n\n# Create data loaders\ntrain_loader, test_loader, src_tokenizer, tgt_tokenizer = create_translation_dataloaders(\n    csv_file=csv_file,\n    src_col='english',  # Replace with your source column name\n    tgt_col='hindi',  # Replace with your target column name\n    batch_size=64,\n    max_length=128,\n    test_size=0.1\n)\n\n# Model parameters (using tokenizer vocab sizes)\nsrc_vocab_size = len(src_tokenizer.word2idx)\ntgt_vocab_size = len(tgt_tokenizer.word2idx)\nd_model = 512\nnum_heads = 8\nnum_layers = 6\nd_ff = 2048\nmax_seq_length = 128\ndropout = 0.1\n\n# Initialize your transformer model (from previous conversation)\nmodel = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    d_model=d_model,\n    num_heads=num_heads,\n    num_layers=num_layers,\n    d_ff=d_ff,\n    max_seq_length=max_seq_length,\n    dropout=dropout\n)\n\n# Training loop adaptation\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n# Training\nmodel.train()\nfor epoch in range(10):\n    total_loss = 0\n    for batch_idx, batch in enumerate(train_loader):\n        src_data = batch['src'].to(device)\n        tgt_data = batch['tgt'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(src_data, tgt_data[:, :-1])\n        \n        # Calculate loss\n        loss = criterion(\n            output.reshape(-1, output.size(-1)), \n            tgt_data[:, 1:].reshape(-1)\n        )\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n    \n    print(f'Epoch {epoch} Average Loss: {total_loss/len(train_loader):.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:55:48.566899Z","iopub.execute_input":"2025-07-31T20:55:48.567203Z","iopub.status.idle":"2025-07-31T20:56:16.303508Z","shell.execute_reply.started":"2025-07-31T20:55:48.567176Z","shell.execute_reply":"2025-07-31T20:56:16.302354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(model, src_sentence, src_tokenizer, tgt_tokenizer, device, max_length=100):\n    model.eval()\n    \n    # Tokenize source sentence\n    src_tokens = [1] + src_tokenizer.encode(src_sentence) + [2]  # Add BOS/EOS\n    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n    \n    # Start with BOS token\n    tgt_tokens = [1]\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n            \n            output = model(src_tensor, tgt_tensor)\n            next_token_logits = output[0, -1, :]\n            next_token = torch.argmax(next_token_logits).item()\n            \n            tgt_tokens.append(next_token)\n            \n            # Stop if EOS token is generated\n            if next_token == 2:  # EOS token\n                break\n    \n    # Decode tokens to text\n    translated_text = tgt_tokenizer.decode(tgt_tokens[1:-1])  # Remove BOS/EOS\n    return translated_text\n\n# Example usage\nsrc_sentence = \"Hello, how are you?\"\ntranslation = translate(model, src_sentence, src_tokenizer, tgt_tokenizer, device)\nprint(f\"Translation: {translation}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:54:05.502378Z","iopub.status.idle":"2025-07-31T20:54:05.502627Z","shell.execute_reply.started":"2025-07-31T20:54:05.502504Z","shell.execute_reply":"2025-07-31T20:54:05.502515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}