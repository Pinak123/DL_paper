{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0a148-8823-40f6-86bb-1632e0745f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import elasticdeform\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec598006-ef81-4c01-bbbf-f4d8fa8a0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution block as in original paper\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Encoder block with maxpool + double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Decoder block with upconv + skip connection + double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle input size differences\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # Concatenate skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d33d9c-0599-46e7-ad8d-5e03598013fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture exactly as in original paper\"\"\"\n",
    "    def __init__(self, n_channels=1, n_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        # Decoder (Expansive Path)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.outc(x)\n",
    "        return self.sigmoid(logits)\n",
    "\n",
    "# Initialize model and check parameters\n",
    "model = UNet(n_channels=1, n_classes=1).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "\n",
    "# Test model with dummy input\n",
    "dummy_input = torch.randn(1, 1, 512, 512).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(f'Input shape: {dummy_input.shape}')\n",
    "    print(f'Output shape: {output.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7928ac-4844-4aa9-b641-cdd47ac429df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"Weighted Binary Cross-Entropy for class imbalance\"\"\"\n",
    "    def __init__(self, pos_weight=1.0):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        inputs = torch.clamp(inputs, 1e-7, 1.0 - 1e-7)\n",
    "        \n",
    "        # Calculate weighted loss\n",
    "        loss = -(self.pos_weight * targets * torch.log(inputs) + \n",
    "                (1 - targets) * torch.log(1 - inputs))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for better segmentation performance\"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combine weighted BCE and Dice loss\"\"\"\n",
    "    def __init__(self, pos_weight=2.0, smooth=1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_loss = WeightedBCELoss(pos_weight)\n",
    "        self.dice_loss = DiceLoss(smooth)\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce = self.bce_loss(inputs, targets)\n",
    "        dice = self.dice_loss(inputs, targets)\n",
    "        return bce + dice\n",
    "\n",
    "# Test loss functions\n",
    "criterion = CombinedLoss(pos_weight=2.0)\n",
    "dummy_pred = torch.sigmoid(torch.randn(1, 1, 512, 512))\n",
    "dummy_target = torch.randint(0, 2, (1, 1, 512, 512)).float()\n",
    "\n",
    "test_loss = criterion(dummy_pred, dummy_target)\n",
    "print(f'Test loss value: {test_loss.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e3252-2881-4a00-b6cf-4fb811865262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellSegmentationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for cell segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, transform=None, augment=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Handle case where files might not exist\n",
    "        if image is None or mask is None:\n",
    "            # Create dummy data for demonstration\n",
    "            image = np.random.randint(0, 255, (512, 512), dtype=np.uint8)\n",
    "            mask = np.random.randint(0, 2, (512, 512), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Resize to 512x512\n",
    "        image = cv2.resize(image, (512, 512))\n",
    "        mask = cv2.resize(mask, (512, 512))\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            image, mask = self.augment_data(image, mask)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def elastic_transform(self, image, mask, alpha=1000, sigma=50):\n",
    "        \"\"\"Apply elastic deformation as in original paper\"\"\"\n",
    "        try:\n",
    "            # Create displacement fields\n",
    "            displacement = np.random.randn(2, *image.shape) * sigma\n",
    "            \n",
    "            # Apply deformation to both image and mask\n",
    "            image_deformed = elasticdeform.deform_grid(image, displacement, alpha=alpha)\n",
    "            mask_deformed = elasticdeform.deform_grid(mask, displacement, alpha=alpha)\n",
    "            \n",
    "            return image_deformed, mask_deformed\n",
    "        except:\n",
    "            # Fallback if elasticdeform fails\n",
    "            return image, mask\n",
    "    \n",
    "    def augment_data(self, image, mask):\n",
    "        \"\"\"Complete augmentation pipeline\"\"\"\n",
    "        # Random rotation\n",
    "        if np.random.random() > 0.5:\n",
    "            angle = np.random.uniform(-180, 180)\n",
    "            image = rotate(image, angle, reshape=False)\n",
    "            mask = rotate(mask, angle, reshape=False)\n",
    "        \n",
    "        # Random flip\n",
    "        if np.random.random() > 0.5:\n",
    "            image = np.fliplr(image)\n",
    "            mask = np.fliplr(mask)\n",
    "        \n",
    "        # Elastic deformation (most important for cells)\n",
    "        if np.random.random() > 0.5:\n",
    "            image, mask = self.elastic_transform(image, mask)\n",
    "        \n",
    "        # Intensity variations\n",
    "        if np.random.random() > 0.5:\n",
    "            image = image * np.random.uniform(0.8, 1.2)\n",
    "            image = np.clip(image, 0, 1)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Create dummy dataset for demonstration\n",
    "def create_dummy_data_paths(num_samples=100):\n",
    "    \"\"\"Create dummy data paths for demonstration\"\"\"\n",
    "    image_paths = [f'dummy_image_{i}.png' for i in range(num_samples)]\n",
    "    mask_paths = [f'dummy_mask_{i}.png' for i in range(num_samples)]\n",
    "    return image_paths, mask_paths\n",
    "\n",
    "# Create datasets\n",
    "train_images, train_masks = create_dummy_data_paths(80)\n",
    "val_images, val_masks = create_dummy_data_paths(20)\n",
    "\n",
    "train_dataset = CellSegmentationDataset(train_images, train_masks, augment=True)\n",
    "val_dataset = CellSegmentationDataset(val_images, val_masks, augment=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "\n",
    "# Test dataset\n",
    "sample_image, sample_mask = train_dataset[0]\n",
    "print(f'Sample image shape: {sample_image.shape}')\n",
    "print(f'Sample mask shape: {sample_mask.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa62ea7-e3a0-4f86-b749-735fe4350824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1):\n",
    "    \"\"\"Calculate Dice coefficient\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "def iou_score(pred, target, smooth=1):\n",
    "    \"\"\"Calculate IoU score\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Apply threshold\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            \n",
    "            dice = dice_coefficient(predictions, masks)\n",
    "            iou = iou_score(predictions, masks)\n",
    "            \n",
    "            dice_scores.append(dice.item())\n",
    "            iou_scores.append(iou.item())\n",
    "    \n",
    "    return np.mean(dice_scores), np.mean(iou_scores)\n",
    "\n",
    "# Test metrics with dummy data\n",
    "dummy_pred = torch.sigmoid(torch.randn(2, 1, 512, 512))\n",
    "dummy_target = torch.randint(0, 2, (2, 1, 512, 512)).float()\n",
    "\n",
    "test_dice = dice_coefficient(dummy_pred, dummy_target)\n",
    "test_iou = iou_score(dummy_pred, dummy_target)\n",
    "\n",
    "print(f'Test Dice coefficient: {test_dice.item():.4f}')\n",
    "print(f'Test IoU score: {test_iou.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc3dc7-da05-4670-91f2-568616a1d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, train_loader, val_loader, device, num_epochs=50):\n",
    "    \"\"\"Training loop for U-Net\"\"\"\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(pos_weight=2.0)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.99, weight_decay=0.0005)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-7)\n",
    "    \n",
    "    # Training tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_dice_scores = []\n",
    "    val_dice_scores = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate Dice score\n",
    "            with torch.no_grad():\n",
    "                dice = dice_coefficient(outputs, masks)\n",
    "                train_dice += dice.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.6f}',\n",
    "                'Dice': f'{dice.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_dice = train_dice / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_dice_scores.append(avg_train_dice)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate Dice score\n",
    "                dice = dice_coefficient(outputs, masks)\n",
    "                val_dice += dice.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.6f}',\n",
    "                    'Dice': f'{dice.item():.4f}'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = val_dice / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_dice_scores.append(avg_val_dice)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.6f}, Train Dice: {avg_train_dice:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.6f}, Val Dice: {avg_val_dice:.4f}')\n",
    "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_unet_cell_seg.pth')\n",
    "            patience_counter = 0\n",
    "            print(f'  -> New best model saved! (Val Loss: {best_val_loss:.6f})')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_dice_scores': train_dice_scores,\n",
    "        'val_dice_scores': val_dice_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d8578-da69-49c1-ba76-bf79072c456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 2  # Small batch size due to memory constraints\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')\n",
    "\n",
    "# Initialize fresh model\n",
    "model = UNet(n_channels=1, n_classes=1).to(device)\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = train_unet(model, train_loader, val_loader, device, num_epochs=20)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a9b82-b1da-4b70-99c1-fde8b77ba0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(history['train_losses'], label='Train Loss', color='blue')\n",
    "    ax1.plot(history['val_losses'], label='Val Loss', color='red')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot Dice scores\n",
    "    ax2.plot(history['train_dice_scores'], label='Train Dice', color='blue')\n",
    "    ax2.plot(history['val_dice_scores'], label='Val Dice', color='red')\n",
    "    ax2.set_title('Training and Validation Dice Score')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Dice Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {history['train_losses'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history['val_losses'][-1]:.6f}\")\n",
    "print(f\"Final Training Dice: {history['train_dice_scores'][-1]:.4f}\")\n",
    "print(f\"Final Validation Dice: {history['val_dice_scores'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e2945-3bfb-406c-bb26-3b3caa7464bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
